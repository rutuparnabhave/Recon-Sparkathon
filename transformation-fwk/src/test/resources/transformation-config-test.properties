hdfs.url=
config.root= ${hdfs.url}/conf
sql.root= ${config.root}/sql
source= sql
write= mem

#transformation steps (referred later as ${transform.step}})
transform.steps= book_entry_interim_loader, store_into_temp_table
# SQL Source SQL common configuration
# sql source is enabled by "source=sql"
sql.path= ${sql.root}/${step.name}.hql
sql.params.SOURCE_TABLE=NOTHING
sql.params.NOTHING=NOTHING

#Kafka consumer properties
kafka.consumer.broker.servers=bootstrap.server
kafka.consumer.group.id=
kafka.consumer.auto.offset.reset=
kafka.consumer.enable.auto.commit=
kafka.consumer.key.deserializer=
kafka.consumer.value.deserializer=

#Kafka producer properties
kafka.producer.bootstrap.servers=not_initialized
kafka.producer.acks=
kafka.producer.batch.size=10011
kafka.producer.key.serializer=
kafka.producer.value.serializer=

# Spark mem store target
mem.table= ${transform.step}_mem
mem.storage_level= MEMORY_ONLY_SER

#transformation properties
transform.class=com.cv.sparkathon.PassThroughTransformationService

book_entry_interim_loader.transform.class=com.cv.sparkathon.GenericSQLTransformationService
book_entry_interim_loader.transform.sql.path= ${sql.root}/book_entry_transformation.hql
book_entry_interim_loader.write= kafka
book_entry_interim_loader.source= sql
book_entry_interim_loader.sql.path= ${sql.root}/book_entry_interim_loader_new.hql
book_entry_interim_loader.sql.params.SOURCE_TABLE= book_entry_final
book_entry_interim_loader.kafka.schema= ${config.root}/schemas/output.schema.json
book_entry_interim_loader.kafka.topic=book_entry_interim_loader-output-topic
book_entry_interim_loader.kafka.producer.bootstrap.servers=book_entry_interim_loader.bootstrap.server
